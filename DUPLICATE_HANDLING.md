# ğŸ”’ Duplicate Detection & Handling

## Overview

The Timesheet OCR system now includes **automatic duplicate detection** to prevent processing the same timesheet multiple times.

---

## ğŸ¯ How It Works

### Built-in DynamoDB Protection

**Primary Key Structure:**
```
Partition Key: ResourceName (e.g., "Nik_Coultas")
Sort Key: DateProjectCode (e.g., "2025-09-29#PJ021931")
```

**This means:**
- Each combination of `Resource + Date + Project` is **unique**
- Uploading the same timesheet twice will **OVERWRITE** the previous entry
- **No true duplicates** can exist in the database

---

## âœ… What Happens When You Re-upload

### Scenario: Upload the same timesheet twice

**First Upload:**
```
File: timesheet_2025-10-01.png
Resource: Nik Coultas
Date Range: Sep 29 - Oct 5, 2025
Projects: PJ021931 (35 hours total)

Result: âœ“ 7 entries created (7 days Ã— 1 project)
```

**Second Upload (Same File):**
```
File: timesheet_2025-10-01.png (uploaded again)
Resource: Nik Coultas
Date Range: Sep 29 - Oct 5, 2025
Projects: PJ021931 (35 hours total)

Detection: âš ï¸  Found 7 existing entries
Action: OVERWRITES previous 7 entries with new data
Result: Still 7 entries (not 14!)
```

---

## ğŸ›¡ï¸ Duplicate Detection Features

### 1. **Automatic Detection**
Lambda automatically checks for existing entries before storing:
- Queries DynamoDB for matching Resource + Date Range
- Logs warning if duplicates found
- Shows which source images created previous entries

### 2. **CloudWatch Logs**
Every upload logs duplicate status:
```
âœ“ No duplicates found - new entries will be created
```
OR
```
âš ï¸  Duplicate detection: Found 7 existing entries from 1 source(s)
   Previous sources: timesheet_old.png
   Will overwrite 7 existing entries
```

### 3. **Response Metadata**
Lambda response includes duplicate information:
```json
{
  "duplicate_info": {
    "was_duplicate": true,
    "overwritten_entries": 7
  }
}
```

---

## ğŸ“Š What Gets Overwritten

When a duplicate is detected and overwritten:

### âœ… **Updated Fields:**
- `Hours` - New hours value
- `SourceImage` - New image filename
- `ProcessingTimestamp` - New timestamp
- `ProcessingTimeSeconds` - New processing time
- `ModelId` - Current model used
- `InputTokens` - New token count
- `OutputTokens` - New token count
- `CostEstimateUSD` - New cost estimate

### âš ï¸  **Lost Information:**
- Previous `SourceImage` name
- Previous `ProcessingTimestamp`
- Previous processing metadata

---

## ğŸ¨ UI Behavior

The Desktop UI shows duplicate warnings in the logs:

```
[12:30:15] Processing 1/1: timesheet_2025-10-01.png
[12:30:16]   â¬†ï¸  Uploading to S3...
[12:30:18]   âœ“ Uploaded
[12:30:19]   ğŸš€ Triggering Lambda function...
[12:30:32]   âœ“ Success!
[12:30:32]     Resource: Nik Coultas
[12:30:32]     Entries Stored: 7
[12:30:32]     âš ï¸  Duplicate: Overwrote 7 existing entries
```

---

## ğŸ” Checking for Duplicates Manually

### Via Desktop UI:
1. Click **"ğŸ“Š View Data"**
2. Look for same Resource + Date + Project combinations
3. Check `SourceImage` column to see which file created each entry

### Via AWS Console:
1. Go to DynamoDB â†’ Tables â†’ `TimesheetOCR-dev`
2. Click **"Explore table items"**
3. Filter by ResourceName and Date to see entries

### Via CLI:
```bash
aws dynamodb query \
  --table-name TimesheetOCR-dev \
  --key-condition-expression "ResourceName = :rn AND begins_with(DateProjectCode, :date)" \
  --expression-attribute-values '{
    ":rn": {"S": "Nik_Coultas"},
    ":date": {"S": "2025-09-29"}
  }'
```

---

## ğŸš¨ Common Scenarios

### Scenario 1: Uploading Corrected Timesheet
**Problem:** Made a mistake in original timesheet, need to upload corrected version

**Solution:** Just upload the corrected timesheet
- âœ… Old entries will be overwritten with correct hours
- âœ… Database stays clean (no duplicates)
- âœ… Latest data is always used

### Scenario 2: Multiple People Same Week
**Problem:** Processing multiple timesheets for same week

**Solution:** No conflicts!
- âœ… Different ResourceName = Different partition key
- âœ… Each person's data is isolated
- âœ… Safe to upload all timesheets for same week

### Scenario 3: Same Person, Different Projects
**Problem:** Person has multiple projects in same week

**Solution:** Works perfectly!
- âœ… Different ProjectCode = Different sort key
- âœ… Each project tracked separately
- âœ… All hours calculated correctly in reports

### Scenario 4: Accidentally Upload Same File Twice
**Problem:** Clicked upload twice on same file

**Solution:** No problem!
- âœ… Second upload overwrites first
- âœ… No duplicate entries created
- âœ… Cost: Only 2Ã— processing cost (minimal)

---

## ğŸ’¡ Best Practices

### âœ… **DO:**
1. **Upload corrected timesheets** - Old data is safely overwritten
2. **Process all timesheets weekly** - Each person/week is isolated
3. **Re-upload if hours change** - Database stays current
4. **Check CloudWatch logs** - See duplicate warnings if concerned

### âŒ **DON'T:**
1. **Worry about duplicates** - System handles it automatically
2. **Manually delete entries** - Just re-upload with correct data
3. **Skip uploads due to duplicate fears** - Overwrites are safe and intentional

---

## ğŸ“ˆ Reporting Impact

### Download Report Button
The "ğŸ“¥ Download Report" button **automatically handles duplicates**:
- âœ… Groups by Resource + Project
- âœ… Sums hours across all dates
- âœ… Uses latest data (if overwrites occurred)
- âœ… No duplicate rows in report

**Example Report Output:**
```csv
Resource Name,Project Code,Project Name,Total Hours
Nik Coultas,PJ021931,(NDA) Fixed Transformation,105.00
```
If same timesheet uploaded 3 times, report still shows `105.00` (not `315.00`)

---

## ğŸ”§ Technical Details

### DynamoDB PutItem Behavior
```python
# This code OVERWRITES if key exists:
table.put_item(Item={
    'ResourceName': 'Nik_Coultas',
    'DateProjectCode': '2025-09-29#PJ021931',
    'Hours': 7.5
})
```

**NOT:**
- âŒ Creates duplicate entry
- âŒ Adds to existing hours
- âŒ Keeps both old and new

**INSTEAD:**
- âœ… Replaces entire item
- âœ… Updates all attributes
- âœ… Maintains single entry

### Source Image Tracking
Each entry stores:
```json
{
  "SourceImage": "timesheet_2025-10-01.png",
  "ProcessingTimestamp": "2025-10-16T12:30:32Z"
}
```

**Limitation:** If you overwrite, you lose track of the original source image.

**Workaround (if needed):** Check CloudWatch Logs for historical processing records.

---

## ğŸ“Š Monitoring Duplicates

### CloudWatch Logs Query
```
filter @message like /Duplicate detection/
| fields @timestamp, @message
| sort @timestamp desc
| limit 20
```

This shows all duplicate detections in the last processing runs.

### DynamoDB Item History
DynamoDB doesn't keep version history by default. If you need audit trail:

**Option 1:** Enable DynamoDB Streams + Lambda to log changes
**Option 2:** Check CloudWatch Logs for processing history
**Option 3:** Keep source images in S3 with timestamps

---

## ğŸ¯ Summary

### Current Behavior:
âœ… **Duplicates are automatically prevented** by DynamoDB key structure
âœ… **Re-uploads overwrite old data** with new values
âœ… **Detection logs warnings** to CloudWatch
âœ… **UI shows duplicate info** in processing logs
âœ… **Reports handle duplicates correctly** through aggregation

### Future Enhancements (Optional):
- [ ] Add DynamoDB Streams to track all changes
- [ ] Create separate audit table for processing history
- [ ] Add UI confirmation dialog before overwriting
- [ ] Store multiple source images per entry (array)
- [ ] Add "revert to previous version" feature

---

**The duplicate handling system is production-ready and requires no user action!** ğŸ‰
